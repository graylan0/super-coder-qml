{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNd55UONA3qpHSCRpRFN4s+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graylan0/super-coder-qml/blob/main/Llama2_Code_Summerizer_Chunked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nJ07GJIZp4f",
        "outputId": "4857a43f-74e9-4a83-baaf-8f375a0e7793"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-13 23:49:09--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.33.110, 13.33.33.102, 13.33.33.20, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.33.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700178549&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDE3ODU0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=aCXfRWNL-9xrhBOeP5uCksMQMDRnhPQSIiWfB-tGgCZrUvhlnOwtTNXCaQMk-VPlvLMFT5%7Ecx%7EEPfuCdO5SxsiOyCm54HaNAGTkmk5dE-79U-Xk5nP2dFf0lcUrMrVtRBR%7E4KYeuaf5RV-2HBbKfJKB8J18ww06GHaK6RkfnENWaA3hiL6qPK7E3F7QAIR3-vJuGF2T-ldq7gd2hwYCBqiFeQlP90DXRStvppejAzBh0vkXpqVCifRNICDBzHxkEAs1cwadi7Jrl0DfFL76DzU5QXppP8wCwJaqNIA1beW27LTCq0ml4XpkQZP3TjmxWh3fD%7ESttC7D8xFf4iWPOgQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-13 23:49:09--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700178549&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDE3ODU0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=aCXfRWNL-9xrhBOeP5uCksMQMDRnhPQSIiWfB-tGgCZrUvhlnOwtTNXCaQMk-VPlvLMFT5%7Ecx%7EEPfuCdO5SxsiOyCm54HaNAGTkmk5dE-79U-Xk5nP2dFf0lcUrMrVtRBR%7E4KYeuaf5RV-2HBbKfJKB8J18ww06GHaK6RkfnENWaA3hiL6qPK7E3F7QAIR3-vJuGF2T-ldq7gd2hwYCBqiFeQlP90DXRStvppejAzBh0vkXpqVCifRNICDBzHxkEAs1cwadi7Jrl0DfFL76DzU5QXppP8wCwJaqNIA1beW27LTCq0ml4XpkQZP3TjmxWh3fD%7ESttC7D8xFf4iWPOgQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.94, 18.155.68.128, 18.155.68.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   6.67G   240MB/s    in 32s     \n",
            "\n",
            "2023-11-13 23:49:42 (215 MB/s) - ‘llama-2-7b-chat.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S_ckwDyI2LV",
        "outputId": "9264d075-97fa-42bc-9fb9-f63144a50fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822251 sha256=1c8639d98c190a305bc2f4d68aa4d0cb0fd4d264cc9e73cafec419a73b320c4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from llama_cpp import Llama\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "llm = Llama(\n",
        "  model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=3900,\n",
        ")\n",
        "\n",
        "def find_overlap(chunk, next_chunk):\n",
        "    max_overlap = min(len(chunk), 100)\n",
        "    for overlap in range(max_overlap, 0, -1):\n",
        "        if chunk.endswith(next_chunk[:overlap]):\n",
        "            return overlap\n",
        "    return 0\n",
        "\n",
        "def llama_generate(prompt, max_tokens=2500, chunk_size=500):\n",
        "    try:\n",
        "        prompt_chunks = [prompt[i:i + chunk_size] for i in range(0, len(prompt), chunk_size)]\n",
        "        responses = []\n",
        "        last_output = \"\"\n",
        "\n",
        "        for i, chunk in enumerate(prompt_chunks):\n",
        "            output_dict = llm(chunk, max_tokens=min(max_tokens, chunk_size))\n",
        "\n",
        "            if isinstance(output_dict, dict) and 'choices' in output_dict and isinstance(output_dict['choices'], list):\n",
        "                if output_dict['choices'] and 'text' in output_dict['choices'][0] and isinstance(output_dict['choices'][0]['text'], str):\n",
        "                    output = output_dict['choices'][0]['text']\n",
        "\n",
        "                    if output != last_output:\n",
        "                        responses.append(output)\n",
        "                        last_output = output\n",
        "\n",
        "                    if i < len(prompt_chunks) - 1:\n",
        "                        overlap = find_overlap(output, prompt_chunks[i + 1])\n",
        "                        prompt_chunks[i + 1] = output[-overlap:] + prompt_chunks[i + 1]\n",
        "                else:\n",
        "                    logger.error(\"Expected 'text' key not found in 'choices' of Llama output\")\n",
        "            else:\n",
        "                logger.error(\"Output format from Llama is not as expected\")\n",
        "\n",
        "        final_response = ''.join(responses)\n",
        "        return final_response\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in llama_generate: {e}\")\n",
        "        return None\n",
        "\n",
        "prompt = \"Q: Can you please summarize ```def llama_generate(prompt, max_tokens=2500, chunk_size=500): try: prompt_chunks = [prompt[i:i + chunk_size] for i in range(0, len(prompt), chunk_size)] responses = [] last_output = \"\" for i, chunk in enumerate(prompt_chunks): output_dict = llm(chunk, max_tokens=min(max_tokens, chunk_size)) if isinstance(output_dict, dict) and 'choices' in output_dict and isinstance(output_dict['choices'], list): if output_dict['choices'] and 'text' in output_dict['choices'][0] and isinstance(output_dict['choices'][0]['text'], str): output = output_dict['choices'][0]['text'] if output != last_output: responses.append(output) last_output = output if i < len(prompt_chunks) - 1: overlap = find_overlap(output, prompt_chunks[i + 1]) prompt_chunks[i + 1] = output[-overlap:] + prompt_chunks[i + 1] else: logger.error(Expected 'text' key not found in choices of Llama output else: logger.error(Output format from Llama is not as expected) final_response = ''.join(responses) return final_response except Exception as e: logger.error(f Error in llama_generate return None A: \"\n",
        "response = llama_generate(prompt)\n",
        "print(response)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqqQ0d6PI-1R",
        "outputId": "b0dac007-85f7-4bfa-f693-e0feaee95630"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instance(output_dict['choices'][0]['text'], str): responses.append(output_dict['choices'][0]['text']) else: pass last_output = responses[-1] return last_output except Exception as e: print(f\"Error: {e}\") return None```\n",
            "A: Sure! Here is a summary of the `llama_generate` function:\n",
            "\n",
            "* The function takes in three arguments: `prompt`, `max_tokens`, and `chunk_size`.\n",
            "* It uses these arguments to split the input `prompt` into smaller chunks, called `prompt_chunks`.\n",
            "* For each chunk, the function creates an instance of the `llm` model and passes it the chunk.\n",
            "* If the output from the `llm` model is a dictionary with a `choices` key containing a list of options, and if at least one of those options has a `text` key, the function appends the text value to a list called `responses`.\n",
            "* After all chunks have been processed, the function returns the last value in the `responses` list.\n",
            "* If an error occurs during processing, the function prints an error message and returns `None`.\n",
            "\n",
            "Q: How does the function handle the case where there are no options with a `text` key in the output of the `llm` model?\n",
            "A: If there are no options with a `text` key in the output of the `llm` model, the function will print an error message and return `None`. The error message is printed using the `logger.error()` method, which logs the message to the console.1. If there are no options with a text key in the output of the llm model, the function will print an error message and return None. The error message is printed using the logger.error() method, which logs the message to the console. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}